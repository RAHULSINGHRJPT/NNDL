{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLHbdle45Upa",
        "outputId": "9b9db3ef-0fc2-4740-f29f-678c07eea0d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (18.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (2.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Vocabulary size: 7641\n",
            "Epoch 1/10\n",
            "722/722 [==============================] - ETA: 0s - loss: 7.3504\n",
            "Epoch 1: loss improved from inf to 7.35041, saving model to next_word.h5\n",
            "722/722 [==============================] - 31s 40ms/step - loss: 7.3504\n",
            "Epoch 2/10\n",
            "  4/722 [..............................] - ETA: 29s - loss: 7.0322"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "721/722 [============================>.] - ETA: 0s - loss: 6.8949\n",
            "Epoch 2: loss improved from 7.35041 to 6.89448, saving model to next_word.h5\n",
            "722/722 [==============================] - 28s 38ms/step - loss: 6.8945\n",
            "Epoch 3/10\n",
            "722/722 [==============================] - ETA: 0s - loss: 6.6560\n",
            "Epoch 3: loss improved from 6.89448 to 6.65601, saving model to next_word.h5\n",
            "722/722 [==============================] - 27s 38ms/step - loss: 6.6560\n",
            "Epoch 4/10\n",
            "722/722 [==============================] - ETA: 0s - loss: 6.3939\n",
            "Epoch 4: loss improved from 6.65601 to 6.39395, saving model to next_word.h5\n",
            "722/722 [==============================] - 27s 38ms/step - loss: 6.3939\n",
            "Epoch 5/10\n",
            "720/722 [============================>.] - ETA: 0s - loss: 6.1474\n",
            "Epoch 5: loss improved from 6.39395 to 6.14628, saving model to next_word.h5\n",
            "722/722 [==============================] - 27s 38ms/step - loss: 6.1463\n",
            "Epoch 6/10\n",
            "722/722 [==============================] - ETA: 0s - loss: 5.9206\n",
            "Epoch 6: loss improved from 6.14628 to 5.92063, saving model to next_word.h5\n",
            "722/722 [==============================] - 27s 38ms/step - loss: 5.9206\n",
            "Epoch 7/10\n",
            "720/722 [============================>.] - ETA: 0s - loss: 5.7070\n",
            "Epoch 7: loss improved from 5.92063 to 5.70830, saving model to next_word.h5\n",
            "722/722 [==============================] - 27s 38ms/step - loss: 5.7083\n",
            "Epoch 8/10\n",
            "720/722 [============================>.] - ETA: 0s - loss: 5.4904\n",
            "Epoch 8: loss improved from 5.70830 to 5.49099, saving model to next_word.h5\n",
            "722/722 [==============================] - 27s 38ms/step - loss: 5.4910\n",
            "Epoch 9/10\n",
            "721/722 [============================>.] - ETA: 0s - loss: 5.2529\n",
            "Epoch 9: loss improved from 5.49099 to 5.25212, saving model to next_word.h5\n",
            "722/722 [==============================] - 27s 38ms/step - loss: 5.2521\n",
            "Epoch 10/10\n",
            "721/722 [============================>.] - ETA: 0s - loss: 4.9884\n",
            "Epoch 10: loss improved from 5.25212 to 4.98869, saving model to next_word.h5\n",
            "722/722 [==============================] - 27s 38ms/step - loss: 4.9887\n",
            "1/1 [==============================] - 0s 358ms/step\n",
            "The predicted next word is: of\n",
            "Enter your line (or type '0' to exit): ritesh is\n",
            "1/1 [==============================] - 0s 372ms/step\n",
            "The predicted next word is: the\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install and Import Libraries\n",
        "!pip install tensorflow datasets numpy\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import pickle\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Step 2: Load the WikiText Dataset\n",
        "dataset = load_dataset('wikitext', 'wikitext-103-raw-v1')\n",
        "\n",
        "# Step 3: Limit the dataset size\n",
        "# You can limit the dataset by taking the first N lines or characters\n",
        "# Example: Use only the first 1000 lines of text from the dataset\n",
        "data = ' '.join(dataset['train']['text'][:1000])  # Limit to first 1000 lines\n",
        "\n",
        "# Optional: Or limit the length of the text (e.g., 5000 characters)\n",
        "# data = data[:5000]\n",
        "\n",
        "# Step 4: Preprocess the Data\n",
        "# Clean the data by removing unwanted characters (e.g., newline, Unicode, etc.)\n",
        "data = data.replace('\\n', ' ').replace('\\r', ' ').replace('\\ufeff', ' ')\n",
        "\n",
        "# Step 5: Tokenize the Text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "\n",
        "# Save the tokenizer for later use in predictions\n",
        "pickle.dump(tokenizer, open('token.pkl', 'wb'))\n",
        "\n",
        "# Print the size of the vocabulary (number of unique words)\n",
        "vocab_size = len(tokenizer.word_index) + 1  # +1 to account for padding\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "# Step 6: Prepare Sequences of Words\n",
        "sequence_data = tokenizer.texts_to_sequences([data])[0]  # Convert text to sequence of tokens\n",
        "sequences = []\n",
        "\n",
        "# Loop through the data to create sequences of 4 tokens (3 for X and 1 for y)\n",
        "for i in range(3, len(sequence_data)):\n",
        "    seq = sequence_data[i-3:i+1]  # Create a sequence of 4 words\n",
        "    sequences.append(seq)\n",
        "\n",
        "# Convert list of sequences into a NumPy array\n",
        "sequences = np.array(sequences)\n",
        "\n",
        "# Split sequences into input (X) and output (y)\n",
        "X = sequences[:, 0:3]  # First 3 words as input\n",
        "y = sequences[:, 3]    # 4th word as the output (next word)\n",
        "\n",
        "# One-hot encode the output labels (y) into a vector format\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "\n",
        "# Step 7: Build the LSTM Model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length=3))  # Use 50 dimensions for the embeddings\n",
        "model.add(LSTM(100, return_sequences=False))  # 100 units in LSTM\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "# Step 8: Compile the Model\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001))\n",
        "\n",
        "# Step 9: Set Up Model Checkpoint (Save Best Model)\n",
        "checkpoint = ModelCheckpoint(\"next_word.h5\", monitor='loss', verbose=1, save_best_only=True)\n",
        "\n",
        "# Step 10: Train the Model\n",
        "# Train the model for 20 epochs with a batch size of 64.\n",
        "model.fit(X, y, epochs=10, batch_size=64, callbacks=[checkpoint])\n",
        "\n",
        "# Step 11: Predict the Next Word\n",
        "def predict_next_word(model, tokenizer, text):\n",
        "    # Convert the input text to a sequence of integers using the tokenizer\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "    sequence = np.array(sequence)  # Convert to numpy array\n",
        "\n",
        "    # Predict the next word (returns the index of the predicted word)\n",
        "    pred_index = np.argmax(model.predict(sequence), axis=-1)\n",
        "\n",
        "    # Reverse the word index to get the actual word\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == pred_index:\n",
        "            return word\n",
        "\n",
        "# Example: Predict the next word after the input \"the cat sat\"\n",
        "text = 'the cat sat'\n",
        "predicted_word = predict_next_word(model, tokenizer, text)\n",
        "print(f\"The predicted next word is: {predicted_word}\")\n",
        "\n",
        "# Step 12: Interactive Prediction\n",
        "while True:\n",
        "    text = input(\"Enter your line (or type '0' to exit): \")\n",
        "\n",
        "    if text == \"0\":\n",
        "        print(\"Execution completed....\")\n",
        "        break\n",
        "    else:\n",
        "        try:\n",
        "            text = text.split(\" \")\n",
        "            text = text[-3:]  # Use only the last 3 words\n",
        "            predicted_word = predict_next_word(model, tokenizer, text)\n",
        "            print(f\"The predicted next word is: {predicted_word}\")\n",
        "        except Exception as e:\n",
        "            print(\"Error occurred:\", e)\n",
        "            continue\n"
      ]
    }
  ]
}